{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning for Inverted Pendulum Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inverted pendulum problem is a classic challenge in the field of control theory and robotics, serving as a fundamental test for various control strategies. In this project, students are required to use reinforcement learning (RL) algorithms to tackle both the single and double inverted pendulum challenges(bonus). You could refer to [inverted_double_pendulum](https://gymnasium.farama.org/environments/mujoco/inverted_double_pendulum/) and [inverted_pendulum](https://gymnasium.farama.org/environments/mujoco/inverted_pendulum/) for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project is divided into two main parts:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Single Inverted Pendulum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Stabilization Task:**\n",
    "\n",
    "The pendulum starts with a slight inclination.\n",
    "The goal is to stabilize the pendulum in the upright position by controlling the movement of the cart.\n",
    "    \n",
    "**2. Swing-Up Task:**\n",
    "\n",
    "The pendulum begins in the downward position.\n",
    "The objective is to swing the pendulum up and stabilize it in the upright position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![part1](part1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Double Inverted Pendulum (Bonus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task involves swinging and stabilizing the double inverted pendulum from an downward initial phase, to any other stable phase(double inverted pendulum has more than phases)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![part2](part2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jude/anaconda3/envs/cartpole/lib/python3.8/site-packages/Cython/Distutils/old_build_ext.py:14: DeprecationWarning: dep_util is Deprecated. Use functions from setuptools instead.\n",
      "  from distutils.dep_util import newer, newer_group\n",
      "/home/jude/anaconda3/envs/cartpole/lib/python3.8/site-packages/Cython/Distutils/old_build_ext.py:14: DeprecationWarning: dep_util is Deprecated. Use functions from setuptools instead.\n",
      "  from distutils.dep_util import newer, newer_group\n",
      "/home/jude/anaconda3/envs/cartpole/lib/python3.8/site-packages/pygame/pkgdata.py:25: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  from pkg_resources import resource_stream, resource_exists\n",
      "/home/jude/anaconda3/envs/cartpole/lib/python3.8/site-packages/pkg_resources/__init__.py:2832: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.normal import Normal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might Need an Agent class and a Policy Network class to help you build the RL framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"Agent that learns to solve the Inverted Pendulum task using a policy gradient algorithm.\n",
    "    The agent utilizes a policy network to sample actions and update its policy based on\n",
    "    collected rewards.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, obs_space_dims: int, action_space_dims: int):\n",
    "        \"\"\"Initializes the agent with a neural network policy.\n",
    "        \n",
    "        Args:\n",
    "            obs_space_dims (int): Dimension of the observation space.\n",
    "            action_space_dims (int): Dimension of the action space.\n",
    "        \"\"\"\n",
    "        self.policy_network = Policy_Network(obs_space_dims, action_space_dims)\n",
    "    \n",
    "    def sample_action(self, state: np.ndarray) -> float:\n",
    "        \"\"\"Samples an action according to the policy network given the current state.\n",
    "        \n",
    "        Args:\n",
    "            state (np.ndarray): The current state observation from the environment.\n",
    "        \n",
    "        Returns:\n",
    "            float: The action sampled from the policy distribution.\n",
    "        \"\"\"\n",
    "        return np.array([0])  # Return the action\n",
    "    \n",
    "    def update(self, rewards, log_probs):\n",
    "        \"\"\"Updates the policy network using the REINFORCE algorithm based on collected rewards and log probabilities.\n",
    "        \n",
    "        Args:\n",
    "            rewards (list): Collected rewards from the environment.\n",
    "            log_probs (list): Log probabilities of the actions taken.\n",
    "        \"\"\"\n",
    "        # The actual implementation of the REINFORCE update will be done here.\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy_Network(nn.Module):\n",
    "    \"\"\"Neural network to parameterize the policy by predicting action distribution parameters.\"\"\"\n",
    "    \n",
    "    def __init__(self, obs_space_dims: int, action_space_dims: int):\n",
    "        \"\"\"Initializes layers of the neural network.\n",
    "        \n",
    "        Args:\n",
    "            obs_space_dims (int): Dimension of the observation space.\n",
    "            action_space_dims (int): Dimension of the action space.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Define the neural network layers here\n",
    "        pass\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Predicts parameters of the action distribution given the state.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): The state observation.\n",
    "        \n",
    "        Returns:\n",
    "            tuple[torch.Tensor, torch.Tensor]: Predicted mean and standard deviation of the action distribution.\n",
    "        \"\"\"\n",
    "        # Implement the prediction logic here\n",
    "        return torch.tensor(0.0), torch.tensor(1.0)  # Example placeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inverted pendulum and inverted double pendulum of gymnasium is loaded by the following code, you need to step into its source codes to see how the models\n",
    " of `\"InvertedPendulum-v4\"` and `\"InvertedDoublePendulum-v4\"` work. You should change the gymnasium codes to help you open the simulation. For example, you need to add the `render_mode` parameter to help you open the simulation window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"InvertedPendulum-v4\")  # Initialize the environment\n",
    "# env = gym.make(\"InvertedDoublePendulum-v4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IN inverted_double_pendulum_v4.py (could not run here)\n",
    "\n",
    "def __init__(self, **kwargs):\n",
    "    observation_space = Box(low=-np.inf, high=np.inf, shape=(11,), dtype=np.float64)\n",
    "    MujocoEnv.__init__(\n",
    "        self,\n",
    "        \"inverted_double_pendulum.xml\",\n",
    "        5,\n",
    "        observation_space=observation_space,\n",
    "        default_camera_config=DEFAULT_CAMERA_CONFIG,\n",
    "        render_mode=\"human\", # you need to add this code\n",
    "        **kwargs,\n",
    "    )\n",
    "    utils.EzPickle.__init__(self, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pendulum_model](pendulum_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can run the following codes to run trainging and simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jude/.local/lib/python3.8/site-packages/glfw/__init__.py:914: GLFWError: (65537) b'The GLFW library is not initialized'\n",
      "  warnings.warn(message, GLFWError)\n"
     ]
    }
   ],
   "source": [
    "wrapped_env = gym.wrappers.RecordEpisodeStatistics(env, 50)  # Wrap the environment to record statistics\n",
    "\n",
    "obs_space_dims = env.observation_space.shape[0]  # Dimension of the observation space\n",
    "action_space_dims = env.action_space.shape[0]  # Dimension of the action space\n",
    "agent = Agent(obs_space_dims, action_space_dims)  # Instantiate the agent\n",
    "\n",
    "total_num_episodes = int(5e3)  # Total number of episodes\n",
    "\n",
    "# Simulation main loop\n",
    "for episode in range(total_num_episodes):\n",
    "    obs, info = wrapped_env.reset()  # Reset the environment at the start of each episode\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.sample_action(obs)  # Sample an action based on the current observation\n",
    "        obs, reward, terminated, truncated, _ = wrapped_env.step(action)  # Take the action in the environment\n",
    "        done = terminated or truncated  # Check if the episode has terminated\n",
    "    # The collection of rewards and log probabilities should happen within the loop.\n",
    "    # agent.update(rewards, log_probs)  # Update the policy based on the episode's experience"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
